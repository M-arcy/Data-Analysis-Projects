{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d812449",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7d978c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7040f7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06b8f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset the data \n",
    "x = iris.drop('species', axis=1)\n",
    "y = iris['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75d11136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split the data \n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=76)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14add63",
   "metadata": {},
   "source": [
    "### When you are doing this normally, you can skip making the single tree; the decision tree is mostly here for comparison. That means that you can run this code immediately after splitting your data into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efc7d17",
   "metadata": {},
   "source": [
    "#### Initial Random Forest Model\n",
    "And at last you are ready to kick it into high gear by creating your random forest model. You'll use the function RandomForestClassifer(), with the arguments n_estimators= to specify how many decision trees you want the random forest to stem from, and of course random_state= just to follow along with this content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07e4f8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=500, random_state=76)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=500, random_state=76)\n",
    "forest.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9895e7",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier?   adding question mark after a function gives possible arguments and info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2d537f",
   "metadata": {},
   "source": [
    "#### This is exactly like when you set up a single tree, except now you specify the number of trees you want to make with n_estimators=, which you have set to 500. This means you will be testing your data with 500 decision trees. If you have a very large dataset, you may want to set n_estimators= smaller to decrease the time it takes to run the RandomForestClassifier() function. However, in general, the higher your n_estimators=, the more accurate your model will be. You'll then fit() to the data. Here is the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "667bb253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=500, random_state=76)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
    "            oob_score=False, random_state=76, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20476648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Marcy\\\\Documents\\\\Entity Coursework\\\\DS106 Machine Learning\\\\Random Forests'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84240896",
   "metadata": {},
   "source": [
    "#### Evaluate Model Fit\n",
    "The final step is to create your prediction set and print a report! Again, you'll make use of both the confusion matrix and the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d02843c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  0  0]\n",
      " [ 0 11  2]\n",
      " [ 0  0 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        19\n",
      "  versicolor       1.00      0.85      0.92        13\n",
      "   virginica       0.87      1.00      0.93        13\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.96      0.95      0.95        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "forestPredictions = forest.predict(x_test)\n",
    "print(confusion_matrix(y_test, forestPredictions))\n",
    "print(classification_report(y_test, forestPredictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b576762",
   "metadata": {},
   "source": [
    "#### Looks like the random forest did better than the decision tree, which is to be expected. All setosa and virginica irises were correctly classified, and only two of the versicolor irises were misclassified as virginica.\n",
    "\n",
    "Now, looking at the classification report, again the random forest model did a better job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fad57105",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    versicolor       1.00      0.85      0.92        13\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "            precision    recall  f1-score   support\n",
    "\n",
    "      setosa       1.00      1.00      1.00        19\n",
    "  versicolor       1.00      0.85      0.92        13\n",
    "   virginica       0.87      1.00      0.93        13\n",
    "\n",
    "   micro avg       0.96      0.96      0.96        45\n",
    "   macro avg       0.96      0.95      0.95        45\n",
    "weighted avg       0.96      0.96      0.96        45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df9563",
   "metadata": {},
   "source": [
    "#### That is it. The new model is 96% accurate, and in general it will become more and more accurate the larger your dataset is. There is 100% accuracy for both setosa and versicolor irises, but only 87% accuracy for virginica. Go forth and explore the Random Forest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66eee38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
