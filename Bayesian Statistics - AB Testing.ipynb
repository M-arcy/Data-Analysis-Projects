{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f07989",
   "metadata": {},
   "source": [
    "#### Bayesian statistics are a branch of stats that make use of probability to test your beliefs against data. In practice, the simplest Bayesian statistics are similar in concept to the basics you learned - probability, the normal distribution, etc. But they go out of their way to change the names of everything! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfcd206",
   "metadata": {},
   "source": [
    "## Bayesian Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a112c5",
   "metadata": {},
   "source": [
    "#### Here's an example to ease you into the Bayesian mindset. You start off with an observation of data. For instance, say you hear a very loud, rushing noise outside. You might come up with a couple different ideas of what is going on, or hypotheses, and those are based on your previous experience. You might have a couple different options: it's a plane making the noise, or it's a tornado. Which is more likely? Well, you know that based on your past experience, tornados make this much noise only once or twice a year when they are very severe. So you're thinking that the plane is more likely.\n",
    "\n",
    "Now add in additional data - you live on an Air Force base. Suddenly, the likelihood that the noise is a fighter jet taking off is much, much higher, and your belief that there's a tornado is almost non-existent. The brilliant thing about Bayesian statistics is that you can continually update your hypotheses based on updated data. You can even compare hypotheses to see which one fits your data better.\n",
    "\n",
    "An important thing to note is that your data should help change your beliefs about the world, but you should not search for data to back up your beliefs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d56987b",
   "metadata": {},
   "source": [
    "## Bayes Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5377bb40",
   "metadata": {},
   "source": [
    "#### Remember back to multiple event probability? Where you either used or or and to combine the probability of multiple things happening? Well, those were great, but they implied independence - that the probability of one of those things did not impact the probability of the other whatsoever. But what happens when your two events are related in any way? For instance, color blindness is much more prevalent in males than in females. So the probability that any one random person is color blind very much depends on their gender. So you can't possibly assume that there is no relation between those two variables!\n",
    "\n",
    "How would you calculate probability in that instance? Enter Bayes theorem! Bayes theorem is a special probability formula that allows you to calculate the likelihood of an event given the likelihood of another event."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d66b2",
   "metadata": {},
   "source": [
    "## Bayes Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2331fee1",
   "metadata": {},
   "source": [
    "#### P(A | B) = P(A)P(B|A)\n",
    "#          ------------\n",
    "#              P(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec7ae75",
   "metadata": {},
   "source": [
    "#### The formula reads like this: The probability of event A given the probability of event B is equal to the probability of event A times the probability of event B given A, divided by the probability of B.\n",
    "\n",
    "You can break it down further. \n",
    "A and B are just two events that are not independent. It's assumed A is the first and B is the second, but it doesn't really matter as long as you stay consistent with your variable assignment throughout the use of the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fff485",
   "metadata": {},
   "source": [
    "#### Lastly you have the pipe symbol |   this means \"given.\"  All in all, this equation is saying if you know the probability of A by itself, and the probability of B by itself, you can figure out how A and B interact. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6490741e",
   "metadata": {},
   "source": [
    "#### Bayesian Reasoning with the Bayes Formula\n",
    "If you want to walk this into the wonderful world of Bayes reasoning that you've just hit upon, you can think of this in terms of observations and beliefs. Substitute for A beliefs, and for B, observations. Now the question becomes, what is the probability of my beliefs being true, given my observations?\n",
    "\n",
    "The pretty cool thing about this is that with Bayes theorem, you can figure out exactly how much your beliefs change because of evidence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22030f6",
   "metadata": {},
   "source": [
    "#### Parts of Bayes Theorem\n",
    "There are three components to Bayes theorem:\n",
    "\n",
    "Posterior probability\n",
    "Likelihood\n",
    "Prior probability\n",
    "You will learn about these components in more detail below!\n",
    "\n",
    "\n",
    "Posterior Probability\n",
    "The posterior probability is the part of the theorem that lets you quantify how strongly you hold beliefs about the data you've observed. The posterior probability is the end result of Bayes' theorem and what you're trying to find out. This is often shortened to just \"posterior.\" No butt jokes, guys!\n",
    "\n",
    "\n",
    "Likelihood\n",
    "The likelihood is the probability of the data given your current beliefs. i.e. How likely is it that x happens? This represents the top left portion of Bayes' theorem - the P(B|A) part.\n",
    "\n",
    "\n",
    "Prior Probability\n",
    "The prior probability is all about the strength of your belief before you see the data. Hence, prior, meaning before! This represents the top right portion of Bayes' theorem - the P(A) part.\n",
    "\n",
    "\n",
    "The Bottom?\n",
    "You may be wondering about the bottom of the equation. Doesn't that get its own special name too? Apparently not, but you're encouraged to give it one. Stormageddon, anyone? But the bottom portion of Bayes' theorem helps normalize the data, so that even if you have a different amount of data in A and B, you can still compare them fairly.\n",
    "\n",
    "\n",
    "An Example\n",
    "You will now calculate the probability that your instructor's a dork, given that she cuddles her statistics book at night. Call \"your instructor's a dork\" A and \"cuddling statistics books\" B.\n",
    "\n",
    "\n",
    "Find the Likelihood\n",
    "You can think of the likelihood in this scenario as the probability that cuddling a statistics book is good evidence that your instructor's a dork. If you are pretty darn certain, then you could make it a probability like 8/10. If you change your mind at any point, well, guess what? That is totally fine! This means P(B|A) = 8/10.\n",
    "\n",
    "\n",
    "Find the Prior\n",
    "\n",
    "First, you need to calculate the prior. Remember, this is just the probability of A. Believe it or not, a survey found that 60% of Americans consider themselves nerds. So you'll use a probability of 6/10 for that. \n",
    "\n",
    "That means: P(A) = 6/10.\n",
    "\n",
    "\n",
    "Calculate the Normalizing Factor P(B)\n",
    "What is P(B)? That's on the bottom! Well that is the probability that someone cuddles their statistics book at night, regardless of whether or not they are a dork. How many people is that? Well, you could take an educated guess based upon the fact that only 11% of people take statistics in a secondary school, and of those, 55% sell them back after the course. That means that only 6.05% (11% * 55%) still own a statistics book after a semester is up. So it's not even very likely that people have statistics books, let alone cuddle them. Maybe 1 in 100 will cuddle a statistics book, and with only 1 in 4 owning them at all...that makes it 6.05% * 1% or .000065.\n",
    "\n",
    "That is one way to go. But if you don't want to estimate it, or it is difficult to estimate it, then you can choose from a standard P(B) setup. Your choices are:\n",
    "\n",
    ".05\n",
    ".01\n",
    ".005\n",
    ".001\n",
    "\n",
    "It is important to note that the smaller the P(B), the larger your posterior probability, or end result, is.\n",
    "\n",
    "Calculate the Posterior\n",
    "\n",
    "Next, you will calculate the posterior. Remember that this is your overall goal! You are ready to solve this bad boy!\n",
    "\n",
    "This is just plug 'n play at this point:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "P(A|B) = (.8 * .6) / .000065\n",
    "\n",
    "P(A|B) = .3 / .000065\n",
    "\n",
    "P(A|B) = 4,615.38\n",
    "\n",
    "That's great! You have a number! But what does it mean? It's really hard to say for sure, especially without a comparison to an alternative hypothesis. It's sort of like comparing machine learning models with AIC - the number itself doesn't matter, just whether it is larger or smaller than other models.\n",
    "\n",
    "Can you guess what you're going to do next?\n",
    "\n",
    "Create and Test Alternative Hypotheses Using Bayes\n",
    "Ok, so one explanation for why your instructor may cuddle her statistics textbook at night is because she doesn't have a pillow. That becomes your new A. A quick internet search shows no relevant results. You can then assume that 99% of people own a pillow, which means that 1% don't.\n",
    "\n",
    "Your new P(A) is now 1/100. And that's probably a high estimate of those who don't own a pillow. How does that change your results?\n",
    "\n",
    "Do some more plug 'n chug!\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "P(A|B) = (.8 * .01) / .000065\n",
    "\n",
    "P(A|B) = .0008 / .000065\n",
    "\n",
    "P(A|B) = 12.31\n",
    "\n",
    "So this means that it is much more likely that your instructor's a dork and not that she doesn't own a pillow. Tada! Relative probability at its finest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce435161",
   "metadata": {},
   "source": [
    "## A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21710ced",
   "metadata": {},
   "source": [
    "#### A/B testing is yet another type of research design, in which you are directly comparing two methods. In practice, if you have means, you'd compare group A and group B with a t-test. But what if you don't have means? What if you have probabilities or percentages? Enter the Bayesian A/B test!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecac2ffc",
   "metadata": {},
   "source": [
    "## Create the Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92baeedb",
   "metadata": {},
   "source": [
    "#### Say you are testing whether your existing recipe for cream cheese frosting (A) is better than your new recipe for cream cheese frosting (B), and that you are testing it at your local bakesale.\n",
    "\n",
    "Your null hypothesis will be that these frostings will be totally equal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ada7e24",
   "metadata": {},
   "source": [
    "## Collect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecac844",
   "metadata": {},
   "source": [
    "## Work the Problem in R using Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd4777",
   "metadata": {},
   "source": [
    "#### finish the A/B testing in R.\n",
    "\n",
    "Monte carlo simulation is a way to simulate the results of something by re-sampling the data you already have. It's based off a little bit of data, but to get the best results, you may want a lot more rows than you have. So use monte carlo simulation to expand things. Kind of like those toy dinosaurs that grow when you pour water over them.\n",
    "\n",
    "The function to do this is rbeta() function, which samples from the probability density function for a binomial distribution. Remember that the binomial distribution is one in which you only have two outcomes. For instance, a) did eat the whole cupcake or b) did not eat the whole cupcake.\n",
    "\n",
    "There are two components of the beta distribution that you'll need to define as variables, in addition to the number of trials you intend to use:\n",
    "\n",
    "alpha: How many times an event happens that you care about\n",
    "beta: How many times an event happens that you don't care about\n",
    "First, assign some variables in R. You'll need a variable to hold onto the priors and the number of trials you want to extend this to. Although you can choose any number of trials you want, here, you'll use 10,000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73137cd",
   "metadata": {},
   "source": [
    "# go to R Bayesian notes file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf09cb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
