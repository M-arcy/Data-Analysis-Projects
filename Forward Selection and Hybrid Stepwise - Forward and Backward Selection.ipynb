{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f8bbabb",
   "metadata": {},
   "source": [
    "#### Dragonfly Statistics youtube video notes: start with intercept only model, using 1. no eplanatory variables in this model. should just turn out to be the mean of mpg in mtcars. start adding regression coefficients in, but only if they're useful and if they make model better. you have to set out how far you have to go, because you probably don't need to use all variables. must specify how big model will be, some limitations.  do that with 'scope=formula(Fitall)'. that's as big as the model will go and no further. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b10c0",
   "metadata": {},
   "source": [
    "#### output is a ranking. law of parsimony and multicollinearity important to consider. best option for modelis to add wt. AIC will drop ifyou add wt. a drop in AIC is improvement in the model. nine more scenarios we can add in variables. add in one by one and reappraise AIC value. keep an eye on none in carats where it moves.when none is top, that's a stopping condition. of the ten, best outcome or subset of the variables is wt, cyl and hp. earlier with backward elimination is different set of variables. AIC is different as well. backward gave better answer than forward selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d0f885",
   "metadata": {},
   "source": [
    "#### R square is the percentage of variation in the DV explained by the IV. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37be515b",
   "metadata": {},
   "source": [
    "### Create original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "440c5e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitstart = lm(mpg~1,data=mtcars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f7dbc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = mpg ~ 1, data = mtcars)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-9.6906 -4.6656 -0.8906  2.7094 13.8094 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)   20.091      1.065   18.86   <2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 6.027 on 31 degrees of freedom\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(fitstart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0c0a2f",
   "metadata": {},
   "source": [
    "#### Notice that the estimate for the intercept is 20.091, which is simply the average mpg for the 28 vehicles in the original mtcars dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "399bc729",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fitall <- lm(mpg~.,data=mtcars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe850c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:  AIC=115.94\n",
      "mpg ~ 1\n",
      "\n",
      "       Df Sum of Sq     RSS     AIC\n",
      "+ wt    1    847.73  278.32  73.217\n",
      "+ cyl   1    817.71  308.33  76.494\n",
      "+ disp  1    808.89  317.16  77.397\n",
      "+ hp    1    678.37  447.67  88.427\n",
      "+ drat  1    522.48  603.57  97.988\n",
      "+ vs    1    496.53  629.52  99.335\n",
      "+ am    1    405.15  720.90 103.672\n",
      "+ carb  1    341.78  784.27 106.369\n",
      "+ gear  1    259.75  866.30 109.552\n",
      "+ qsec  1    197.39  928.66 111.776\n",
      "<none>              1126.05 115.943\n",
      "\n",
      "Step:  AIC=73.22\n",
      "mpg ~ wt\n",
      "\n",
      "       Df Sum of Sq    RSS    AIC\n",
      "+ cyl   1    87.150 191.17 63.198\n",
      "+ hp    1    83.274 195.05 63.840\n",
      "+ qsec  1    82.858 195.46 63.908\n",
      "+ vs    1    54.228 224.09 68.283\n",
      "+ carb  1    44.602 233.72 69.628\n",
      "+ disp  1    31.639 246.68 71.356\n",
      "<none>              278.32 73.217\n",
      "+ drat  1     9.081 269.24 74.156\n",
      "+ gear  1     1.137 277.19 75.086\n",
      "+ am    1     0.002 278.32 75.217\n",
      "\n",
      "Step:  AIC=63.2\n",
      "mpg ~ wt + cyl\n",
      "\n",
      "       Df Sum of Sq    RSS    AIC\n",
      "+ hp    1   14.5514 176.62 62.665\n",
      "+ carb  1   13.7724 177.40 62.805\n",
      "<none>              191.17 63.198\n",
      "+ qsec  1   10.5674 180.60 63.378\n",
      "+ gear  1    3.0281 188.14 64.687\n",
      "+ disp  1    2.6796 188.49 64.746\n",
      "+ vs    1    0.7059 190.47 65.080\n",
      "+ am    1    0.1249 191.05 65.177\n",
      "+ drat  1    0.0010 191.17 65.198\n",
      "\n",
      "Step:  AIC=62.66\n",
      "mpg ~ wt + cyl + hp\n",
      "\n",
      "       Df Sum of Sq    RSS    AIC\n",
      "<none>              176.62 62.665\n",
      "+ am    1    6.6228 170.00 63.442\n",
      "+ disp  1    6.1762 170.44 63.526\n",
      "+ carb  1    2.5187 174.10 64.205\n",
      "+ drat  1    2.2453 174.38 64.255\n",
      "+ qsec  1    1.4010 175.22 64.410\n",
      "+ gear  1    0.8558 175.76 64.509\n",
      "+ vs    1    0.0599 176.56 64.654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = mpg ~ wt + cyl + hp, data = mtcars)\n",
       "\n",
       "Coefficients:\n",
       "(Intercept)           wt          cyl           hp  \n",
       "   38.75179     -3.16697     -0.94162     -0.01804  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "step(fitstart, direction = \"forward\", scope=(formula(Fitall)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a176a0",
   "metadata": {},
   "source": [
    "#### with a model including wt, cyl, and hp, you are done. There is no point in adding any more terms, because according to the table, the AIC will actually increase if you do. Do the same thing you did for the backward elimination model: build a model that only contains wt, cyl, and hp. You called it fitsome in the backward elimination model; call this one fitsome2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d06bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitsome <- lm(mpg~wt+cyl+hp, data=mtcars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89df4cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = mpg ~ wt + cyl + hp, data = mtcars)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-3.9290 -1.5598 -0.5311  1.1850  5.8986 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) 38.75179    1.78686  21.687  < 2e-16 ***\n",
       "wt          -3.16697    0.74058  -4.276 0.000199 ***\n",
       "cyl         -0.94162    0.55092  -1.709 0.098480 .  \n",
       "hp          -0.01804    0.01188  -1.519 0.140015    \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 2.512 on 28 degrees of freedom\n",
       "Multiple R-squared:  0.8431,\tAdjusted R-squared:  0.8263 \n",
       "F-statistic: 50.17 on 3 and 28 DF,  p-value: 2.184e-11\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(fitsome)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aa1eca",
   "metadata": {},
   "source": [
    "#### When you did the backward elimination model, you compared the initial model (all ten predictors) with the optimized model, and concluded that the models are similar as far as R2 is concerned, so the model with just 3 predictors seems preferable based upon simplicity alone. For the forward selection model, there really is no starting point sort of model. You just used the average mpg to get a starting point without any predictor variables.\n",
    "\n",
    "Nonetheless, take a look at the specifics of the forward selection model shown above:\n",
    "\n",
    "The multiple R2 = 0.8431. The practical interpretation of this is that the model explains 84.31% of the variation in the mpg variable, and there is another 15.69% of the variation that can be chalked up to noise or random error.\n",
    "There is an adjusted R2 = 0.8263. The \"adjustment\" is a modification that is supposed to take into account the number of terms in the model. For your purposes, you are more interested in the adjusted R2 than the multiple R2.\n",
    "There is an F-statistic (50.17 with 3 and 28 degrees of freedom) and a p value of 0.00000000002184 (this is represented in scientific notation in R) which indicates that the model is better than no model at all, because the p value is less than 0.05.\n",
    "The R2 for the backward elimination model and the forward selection model are similar. So, which one is better? One could argue that the AIC for the backward elimination model is slightly lower, so it must be better. But for all practical purposes, they are essentially the same. However, did you notice the terms in the predictor model are not the same? That is actually a pretty common result. The truth is that in many cases, there might not just be a single 'best' model that is far superior to the 'second best' model, but there might be a cluster of models that are essentially equally good.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8241fd0",
   "metadata": {},
   "source": [
    "## Hybrid Stepwise - Forward and Backward Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b287f36",
   "metadata": {},
   "source": [
    "#### video notes  fwd selection is intercept only. response variables get added one at a time only if they improve the model. change forward to both.same scope.  best variable to add to the model is lowest AIC.note plus and minus signs in output. minus is it can get taken out again. model might realize it will make model better. multicollinearity and law of parsimony.at the top of the list might be a minus, as in remove some variable that was added previously and variable selection procedure thinks it is time to take it out.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5665ae82",
   "metadata": {},
   "source": [
    "#### model is similar to forward selection model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29404565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:  AIC=115.94\n",
      "mpg ~ 1\n",
      "\n",
      "       Df Sum of Sq     RSS     AIC\n",
      "+ wt    1    847.73  278.32  73.217\n",
      "+ cyl   1    817.71  308.33  76.494\n",
      "+ disp  1    808.89  317.16  77.397\n",
      "+ hp    1    678.37  447.67  88.427\n",
      "+ drat  1    522.48  603.57  97.988\n",
      "+ vs    1    496.53  629.52  99.335\n",
      "+ am    1    405.15  720.90 103.672\n",
      "+ carb  1    341.78  784.27 106.369\n",
      "+ gear  1    259.75  866.30 109.552\n",
      "+ qsec  1    197.39  928.66 111.776\n",
      "<none>              1126.05 115.943\n",
      "\n",
      "Step:  AIC=73.22\n",
      "mpg ~ wt\n",
      "\n",
      "       Df Sum of Sq     RSS     AIC\n",
      "+ cyl   1     87.15  191.17  63.198\n",
      "+ hp    1     83.27  195.05  63.840\n",
      "+ qsec  1     82.86  195.46  63.908\n",
      "+ vs    1     54.23  224.09  68.283\n",
      "+ carb  1     44.60  233.72  69.628\n",
      "+ disp  1     31.64  246.68  71.356\n",
      "<none>               278.32  73.217\n",
      "+ drat  1      9.08  269.24  74.156\n",
      "+ gear  1      1.14  277.19  75.086\n",
      "+ am    1      0.00  278.32  75.217\n",
      "- wt    1    847.73 1126.05 115.943\n",
      "\n",
      "Step:  AIC=63.2\n",
      "mpg ~ wt + cyl\n",
      "\n",
      "       Df Sum of Sq    RSS    AIC\n",
      "+ hp    1    14.551 176.62 62.665\n",
      "+ carb  1    13.772 177.40 62.805\n",
      "<none>              191.17 63.198\n",
      "+ qsec  1    10.567 180.60 63.378\n",
      "+ gear  1     3.028 188.14 64.687\n",
      "+ disp  1     2.680 188.49 64.746\n",
      "+ vs    1     0.706 190.47 65.080\n",
      "+ am    1     0.125 191.05 65.177\n",
      "+ drat  1     0.001 191.17 65.198\n",
      "- cyl   1    87.150 278.32 73.217\n",
      "- wt    1   117.162 308.33 76.494\n",
      "\n",
      "Step:  AIC=62.66\n",
      "mpg ~ wt + cyl + hp\n",
      "\n",
      "       Df Sum of Sq    RSS    AIC\n",
      "<none>              176.62 62.665\n",
      "- hp    1    14.551 191.17 63.198\n",
      "+ am    1     6.623 170.00 63.442\n",
      "+ disp  1     6.176 170.44 63.526\n",
      "- cyl   1    18.427 195.05 63.840\n",
      "+ carb  1     2.519 174.10 64.205\n",
      "+ drat  1     2.245 174.38 64.255\n",
      "+ qsec  1     1.401 175.22 64.410\n",
      "+ gear  1     0.856 175.76 64.509\n",
      "+ vs    1     0.060 176.56 64.654\n",
      "- wt    1   115.354 291.98 76.750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = mpg ~ wt + cyl + hp, data = mtcars)\n",
       "\n",
       "Coefficients:\n",
       "(Intercept)           wt          cyl           hp  \n",
       "   38.75179     -3.16697     -0.94162     -0.01804  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "step(fitstart, direction = \"both\", scope=formula(Fitall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7af289",
   "metadata": {},
   "source": [
    "#### Hierarchical Regression\n",
    "In hierarchical regression, you get to pick the variables that are being added or removed next, which allows you to make statements about how much variance is added \"over and above\" other variables. This approach can be quite useful especially when answering stakeholder questions about what variables are most important.\n",
    "\n",
    "Summary\n",
    "Stepwise regression has three basic approaches - backward elimination, forward selection, and a combination of the two.\n",
    "Backward elimination creates a model with all predictor variables included, and then removes them one at a time until the model is optimized.\n",
    "Forward selection starts with no predictor variables included, and adds them one at a time until the model is optimized.\n",
    "The combination approach starts with no predictor terms. Every time a term is either added or removed from the model, it is compared with the quality of all other models that either add a single predictor or remove a single predictor until the model is optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e226905c",
   "metadata": {},
   "source": [
    "#### Key Terms\n",
    "Below is a list and short description of the important keywords learned in this lesson. Please read through and go back and review any concepts you do not fully understand. Great Work!\n",
    "\n",
    "Keyword\tDescription\n",
    "Stepwise Regression \tA process to determine exactly what variables make up the best-fitting regression model.\n",
    "\n",
    "Multiple Regression\t A regression with multiple independent variables.\n",
    "\n",
    "Overfitting\t When a model fits the data so well that it won't fit future data.\n",
    "\n",
    "Principle of Parsimony\tWhen all other things are equal, go with the simplest explanation.\n",
    "\n",
    "Backward Elimination\tStarting with all the predictors and removing them one at a time to find the best-fitting model.\n",
    "\n",
    "Forward Selection\tStarting with no predictors and adding them one at a time to find the best-fitting model.\n",
    "\n",
    "Hybrid Stepwise Regression\tStarting with no predictors and adding or removing them one at a time to find the best-fitting model.\n",
    "\n",
    "Akaike Information Criteria (AIC)\tAn indicator of model fit quality. The smaller, the better.\n",
    "\n",
    "\n",
    "Key R Code\n",
    "Keyword\tDescription\n",
    "step()\tA function to perform stepwise regression.\n",
    "\n",
    "direction=\"backward\"\tAn argument to step() that performs backward elimination.\n",
    "\n",
    "direction=\"forward\"\tAn argument to step() that performs forward selection.\n",
    "\n",
    "scope=\tAn argument to step() that specifies what model you are moving towards.\n",
    "\n",
    "direction=\"both\"\tAn argument to step() that specifies hybrid stepwise regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c0feed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
